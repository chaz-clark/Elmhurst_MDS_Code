---
title: "Regression - my data Chaz Clark"
output: html_notebook
name: Chaz Clark
---
```{r}
library(arules)
library(Matrix)
library(ROCR)
library(gplots)
```

```{r}
pairs(~Open + Close + High + Low + Volume, data=msft.us, main="Scatterplot Matrix")
```


```{r}
#7.1 Loading the Plums Data
##Make sure to set directory to location of downloaded file or specify path in load call
#load("C://Users/e0005267/OneDrive - Elmhurst College/MDS/MDS 556 19SP/psub.RData")
msft.us <- within(msft.us, Index <- gsub('[[:punct:] ]+','',msft.us$Date))
dtrain <- subset(msft.us, Index < 20170101, select=Open:Volume	)
dtest <- subset(msft.us, Index >= 20170101, select=Open:Volume	)
model <- lm(Open ~ Volume, data=dtrain)
dtest$predOpen <- predict(model, newdata= dtest)
dtrain$predOpen <- predict(model, newdata = dtrain)
head(dtest$predOpen)
```

```{r}
#7.2 PLotting Log Income as a Function of Predicted Log Income
#install.packages("ggplot2")
library(ggplot2)
ggplot(data=dtest, aes(x=predOpen, y=log(Open, base=10))) + 
  geom_point(alpha=0.2, color="black") +
  geom_smooth(aes(x=predOpen,
                  y=log(Open, base=10)), color="black") + 
  geom_line(aes(x=log(Open, base=10),
                y=log(Open, base=10)), color="blue", linetype=2)
```
```{r}
#7.3 PLotting residuals Income as a Function of Predicted Log Income
ggplot(data=dtest, aes(x=predOpen,
                       y=predOpen-log(Open,base=10))) + 
  geom_point(alpha=.2, color="black") +
  geom_smooth(aes(x=predOpen,
                  y=predOpen-log(Open,base=10)),
              color="black")
```
```{r}
#7.4 Computing R-squared
rsq <- function(y,f) {1 - sum((y-f)^2)/sum((y-mean(y))^2) }
rsq(log(dtrain$Open, base=10), predict(model, newdata=dtrain))
rsq(log(dtest$Open, base=10), predict(model, newdata=dtest))
```
```{r}
#7.5 Calculating Root Mean Squared Error
rmse<- function(y,f) {sqrt(mean( (y-f)^2 )) }
rmse(log(dtrain$Open, base=10), predict(model, newdata=dtrain))
rmse(log(dtest$Open, base=10), predict(model, newdata=dtest))
```
```{r}
#7.6 Summarizing Resifuals
summary(log(dtrain$Open, base=10) - predict (model, newdata=dtrain))
summary(log(dtest$Open, base=10) - predict (model, newdata=dtest))
```
```{r}
#7.7 Loading the CDC Data
##Make sure to set directory to location of downloaded file or specify path in load call
#load('NatalRiskData.rData')
train <- subset(msft.us, Index < 20170101, select=Open:Volume	)
test <- subset(msft.us, Index >= 20170101, select=Open:Volume	)
```

```{r}
#7.8 Building the model Formula
complications <- c("Open","Close")
riskfactors <- c("High","Low","Volume")
y <- "atRisk"
x <- c(complications,
       riskfactors)
fmla<- paste(y, paste(x, collapse="+"), sep="~")
```

```{r}
print(fmla)
model <- glm(fmla, data=train, family=binomial(link="logit"))
```

```{r}
#7.10 Applying the Logistic Regression Model
train$pred <- predict(model, newdata=train, type="response")
test$pred <- predict(model, newdata=test, type="response")
```

```{r}
#7.11 PLotting Distribution of Prediction scores grouped by known outcomes
library(ggplot2)
ggplot(train, aes(x=pred, color=atRisk, linetype=atRisk)) + geom_density()
```
```{r}
#7.12 Exploring Modeling trade-offs
library(ROCR)
library(grid)
predObj <- prediction(train$pred, train$atRisk)
precObj <- performance(predObj, measure = 'prec')
recObj <- performance(predObj, measure='rec')

precision <- (precObj@y.values)[[1]]
prec.x <- (precObj@x.values)[[1]]
recall <- (recObj@y.values)[[1]]

rocFrame <- data.frame(threshold=prec.x, precision=precision, recall=recall)

nplot <- function(plist) {
  n <- length(plist)
  grid.newpage()
  pushViewport(viewport(layout=grid.layout(n,1)))
  vplayout= function(x,y) {viewport(layout.pos.row=x,  layout.pos.col=y)}
  for (i in 1:n) {
    print(plist[[i]], vp=vplayout(i,1))
  }
}
 
pnull<- mean(as.numeric(train$atRisk))

p1 <- ggplot(rocFrame, aes(x=threshold)) +
  geom_line(aes(y=precision/pnull)) +
  coord_cartesian(xlim = c(0,.05), ylim = c(0,10) )
  
p2 <- ggplot(rocFrame, aes(x=threshold)) +
  geom_line(aes(y=recall)) + 
  coord_cartesian(xlim = c(0,.05) )

nplot(list(p1,p2))
```

```{r}
#7.13 Evaluating our Chosen Model
ctab.test <- table(pred=test$pred>0.02, atRisk= test$atRisk)
ctab.test
precision <- ctab.test[2,2]/sum(ctab.test[2,])
precision
recall <- ctab.test[2,2]/sum(ctab.test[,2])
recall
enrich<- precision/mean(as.numeric(test$atRisk))
enrich
```

```{r}
#7.14 The Model Coefficients
coefficients(model)
```
```{r}
#7.15 The model Summary
summary(model)
```
```{r}
#7.16 Calculating Deviance Residuals
pred <- predict(model, newdata = train, type = 'response')
  llcomponents <- function(y, py) {
  y*log(py) + (1-y)*log(1-py)
}

edev <- sign(as.numeric(train$atRisk) - pred) *
  sqrt(-2*llcomponents(as.numeric(train$atRisk), pred))

summary(edev)
```

```{r}
#7.17 Computing Deviance
loglikelihood <- function(y,py) {
  sum(y * log(py) + (1-y)*log(1-py))
}

pnull<- mean(as.numeric(train$atRisk))

null.dev<- -2*loglikelihood(as.numeric(train$atRisk), pnull)
pnull

null.dev
model$null.deviance

pred<- predict(model, newdata=train, type="response")
resid.dev<- -2*loglikelihood(as.numeric(train$atRisk), pred)
resid.dev

model$deviance

testy<- as.numeric(test$atRisk)
testpred<- predict(model, newdata=test, type = "response")

pnull.test<- mean(testy)

null.dev.test<- -2*loglikelihood(testy, pnull.test)

resid.dev.test<- -2*loglikelihood(testy, testpred)

pnull.test
null.dev.test
resid.dev.test
```
```{r}
#7.18 Calculating the Significance of the observed fit
df.null <- dim(train)[[1]] -1
df.model <- dim(train)[[1]] - length(model$coefficients)

df.null
df.model

delDev<- null.dev - resid.dev
deldf<- df.null - df.model
p<- pchisq(delDev, deldf, lower.tail = F)

delDev
deldf
p
```
```{r}
#7.19 Calculating the Pseudo R-Squared
pr2<- 1-(resid.dev/null.dev)

print(pr2)

pr2.test<- 1-(resid.dev.test/null.dev.test)
print(pr2.test)
```
```{r}
#7.20 Calculating the Akaike Information Criterion
aic <- 2*(length(model$coefficients) -
            loglikelihood(as.numeric(train$atRisk), pred))
aic
```

